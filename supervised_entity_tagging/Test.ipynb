{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (4.46.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (1.19.4)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2021.10.23-cp37-cp37m-macosx_10_9_x86_64.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (1.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (20.4)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-macosx_10_11_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.0-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.1.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->transformers) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Collecting click\n",
      "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: regex, packaging, click, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\n",
      "Successfully installed click-8.0.3 huggingface-hub-0.0.19 packaging-21.0 regex-2021.10.23 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.dataloader import construct_dataloaders, _reconstruct_input_labels\n",
    "from utils.utils import create_optimizer_and_scheduler\n",
    "from utils.options import parse_arguments\n",
    "\n",
    "from models.net import EntityTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    opts = parse_arguments()\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{opts.gpu}\"\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    torch.manual_seed(opts.seed)\n",
    "    np.random.seed(opts.seed)\n",
    "    if opts.gpu.count(\",\") > 0:\n",
    "        opts.batch_size = opts.batch_size * (opts.gpu.count(\",\")+1)\n",
    "        opts.eval_batch_size = opts.eval_batch_size * (opts.gpu.count(\",\")+1)\n",
    "    loaders = construct_dataloaders(opts.root, opts.model_name, opts.batch_size, opts.num_workers, opts.seed)\n",
    "\n",
    "    model = EntityTagger(\n",
    "        nclass=len(loaders[\"train\"].dataset.label2id),\n",
    "        model_name=opts.model_name\n",
    "    )\n",
    "\n",
    "    if opts.gpu.count(\",\") > 0:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() and (not opts.no_gpu) else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    if not opts.test_only: # you need to add code to load model if you only want to run test\n",
    "        optimizer, scheduler = create_optimizer_and_scheduler(model, opts.learning_rate, opts.decay, opts.warmup_step, len(loaders[\"train\"]) * opts.train_epoch)\n",
    "\n",
    "        # this is just training on a fixed number of epochs. You need to implement yourself if you want to select the best checkpoints according to the dev set performance.\n",
    "        for epoch in range(opts.train_epoch):\n",
    "            iterator = tqdm(loaders[\"train\"])\n",
    "            epoch_loss = 0.\n",
    "            for idx, (encodings, labels) in enumerate(iterator):\n",
    "                try:\n",
    "                    encodings = encodings.to(device)\n",
    "                except Exception as e:\n",
    "                    encodings = {key: val.to(device) for key, val in encodings.items()}\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(encodings, labels)\n",
    "                optimizer.zero_grad()\n",
    "                outputs[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                epoch_loss += outputs[\"loss\"].item()\n",
    "                iterator.set_postfix({\"loss\": epoch_loss / (idx + 1)})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        iterator = tqdm(loaders[\"test\"])\n",
    "        epoch_loss = 0.\n",
    "        predictions = []\n",
    "        test_dataset = loaders[\"test\"].dataset\n",
    "        for idx, (encodings, labels) in enumerate(iterator):\n",
    "            try:\n",
    "                inputs = encodings.to(device)\n",
    "            except Exception as e:\n",
    "                inputs = {key: val.to(device) for key, val in encodings.items()}\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs, labels)\n",
    "            encodings = test_dataset.collate_fn(test_dataset.data[idx * opts.batch_size: (idx+1) * opts.batch_size])[0]\n",
    "            prediction_labels = _reconstruct_input_labels(encodings, outputs[\"prediction\"], loaders[\"test\"].dataset.id2label)\n",
    "            predictions.extend(prediction_labels)\n",
    "            epoch_loss += outputs[\"loss\"].item()\n",
    "            iterator.set_postfix({\"loss\": epoch_loss / (idx + 1)})\n",
    "    outputs = loaders[\"test\"].dataset.dumps_outputs(predictions)\n",
    "    with open(os.path.join(opts.log_dir, \"test_output.txt\"), \"wt\") as fp:\n",
    "        fp.write(outputs)\n",
    "    torch.save(model.state_dict(), os.path.join(opts.log_dir, \"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--root ROOT] [--batch-size BATCH_SIZE]\n",
      "                             [--num-workers NUM_WORKERS] [--no-gpu]\n",
      "                             [--gpu GPU] [--max-grad-norm MAX_GRAD_NORM]\n",
      "                             [--learning-rate LEARNING_RATE] [--decay DECAY]\n",
      "                             [--warmup-step WARMUP_STEP] [--seed SEED]\n",
      "                             [--log-dir LOG_DIR] [--model-name MODEL_NAME]\n",
      "                             [--train-epoch TRAIN_EPOCH] [--test-only]\n",
      "                             [--clean-log-dir]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/chiragrastogi/Library/Jupyter/runtime/kernel-7ee4b5b2-635c-4fa9-a2c0-9e6ad6aad54a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3425: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
